{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "606986be-fd43-4b0f-b69b-02250e57e4b0",
   "metadata": {},
   "source": [
    "### Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae9d6495-af97-405d-b4d6-63b322cb82d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U torch datasets transformers tensorflow langchain playwright html2text sentence_transformers faiss-cpu\n",
    "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 trl==0.4.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacdda41-708b-4af8-88a9-5056cbd08bf4",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505ca9a3-8c27-442e-bca6-154a65186d01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_transformers import Html2TextTransformer\n",
    "from langchain.document_loaders import AsyncChromiumLoader\n",
    "\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180f8d4e-a8bf-4389-b046-9827b310a3b3",
   "metadata": {},
   "source": [
    "### Load quantized Mistal 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f94e97-7e58-4253-9376-73af6f36e139",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Tokenizer\n",
    "#################################################################\n",
    "\n",
    "model_name='mistralai/Mistral-7B-Instruct-v0.2'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "#################################################################\n",
    "# bitsandbytes parameters\n",
    "#################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "#################################################################\n",
    "# Set up quantization config\n",
    "#################################################################\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "#################################################################\n",
    "# Load pre-trained config\n",
    "#################################################################\n",
    "mistral_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fb199a-a537-4bd7-9888-d43a84c8ff69",
   "metadata": {},
   "source": [
    "### Count number of trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91d2a86e-69e8-496f-b388-853168537c20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 262410240\n",
      "all model parameters: 3752071168\n",
      "percentage of trainable model parameters: 6.99%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(mistral_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a38c760-f5c8-49c6-9c0c-80719557fee5",
   "metadata": {},
   "source": [
    "### Build Mistral text generation pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c613429-9e6c-4a1e-bc9c-579eb152434b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "standalone_query_generation_pipeline = pipeline(\n",
    " model=mistral_model,\n",
    " tokenizer=tokenizer,\n",
    " task=\"text-generation\",\n",
    " temperature=0.0,\n",
    " repetition_penalty=1.1,\n",
    " return_full_text=True,\n",
    " max_new_tokens=1000,\n",
    ")\n",
    "standalone_query_generation_llm = HuggingFacePipeline(pipeline=standalone_query_generation_pipeline)\n",
    "\n",
    "response_generation_pipeline = pipeline(\n",
    " model=mistral_model,\n",
    " tokenizer=tokenizer,\n",
    " task=\"text-generation\",\n",
    " temperature=0.2,\n",
    " repetition_penalty=1.1,\n",
    " return_full_text=True,\n",
    " max_new_tokens=1000,\n",
    ")\n",
    "response_generation_llm = HuggingFacePipeline(pipeline=response_generation_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a07789-78f5-498c-987b-9ed3eb459fe6",
   "metadata": {},
   "source": [
    "### Load and chunk documents. Load chunked documents into FAISS index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bd0398-f7b5-479c-9ed9-2341e4701fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!playwright install \n",
    "!playwright install-deps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79a2e41f-aee3-47ff-92a1-74970f3b313a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Articles to index\n",
    "articles = [\"https://www.gamespot.com/gallery/the-biggest-game-releases-of-december-2023/2900-4962/\",]\n",
    "\n",
    "# Scrapes the blogs above\n",
    "loader = AsyncChromiumLoader(articles)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff328fea-b7c7-4ca3-915c-39c0ebaa2f7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 4148, which is longer than the specified 800\n",
      "Created a chunk of size 898, which is longer than the specified 800\n"
     ]
    }
   ],
   "source": [
    "# Converts HTML to plain text \n",
    "html2text = Html2TextTransformer()\n",
    "docs_transformed = html2text.transform_documents(docs)\n",
    "\n",
    "# Chunk text\n",
    "text_splitter = CharacterTextSplitter(chunk_size=800, \n",
    "                                      chunk_overlap=0)\n",
    "chunked_documents = text_splitter.split_documents(docs_transformed)\n",
    "\n",
    "# Load chunked documents into the FAISS index\n",
    "db = FAISS.from_documents(chunked_documents, \n",
    "                          HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2'))\n",
    "\n",
    "retriever = db.as_retriever(k = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d54a0b-bf6c-4a24-b888-4c3283b9ccf6",
   "metadata": {},
   "source": [
    "### Create PromptTemplate and LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "223cf384-5c03-4399-aa64-4607f5bd2de2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.schema import format_document\n",
    "from langchain_core.messages import get_buffer_string\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fd592e9-8f84-400e-963f-2a449e41f1ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_template = \"\"\"\n",
    "[INST] \n",
    "Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, that can be used to query a FAISS index. This query will be used to retrieve documents with additional context. \n",
    "\n",
    "Here are some important examples. \n",
    "\n",
    "If you do not see any chat history, you MUST return the \"Follow Up Input\" as is:\n",
    "\n",
    "```\n",
    "Chat History:\n",
    "\n",
    "Follow Up Input: What day did Avatar release?\n",
    "Standalone Question:\n",
    "What day did Avatar: Frontiers of Pandora release?\n",
    "```\n",
    "\n",
    "If this is the second question onwards, you should properly rephrase the question like this:\n",
    "\n",
    "```\n",
    "Chat History:\n",
    "Human: What day did Avatar release?\n",
    "AI: \n",
    "Avatar: Frontiers of Pandora released on December 7th.\n",
    "\n",
    "Follow Up Input: On what platform was it released?\n",
    "Standalone Question:\n",
    "On what platform was Avatar: Frontiers of Pandora realeased?\n",
    "```\n",
    "\n",
    "Now, with those examples, here is the actual chat history and input question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\n",
    "[your response here]\n",
    "[/INST] \n",
    "\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbb05ea7-cc5b-4e3a-978c-cecd65d6488c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "[INST] \n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "[/INST] \n",
    "\"\"\"\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c848fc35-37b5-47d5-98b4-b34b6f4c7155",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "\n",
    "def _combine_documents(\n",
    "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
    "):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b18f66e5-684e-481b-a169-6289297dca6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    " return_messages=True, output_key=\"answer\", input_key=\"question\"\n",
    ")\n",
    "\n",
    "# First we add a step to load memory\n",
    "# This adds a \"memory\" key to the input object\n",
    "loaded_memory = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",
    ")\n",
    "# Now we calculate the standalone question\n",
    "standalone_question = {\n",
    "    \"standalone_question\": {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),\n",
    "    }\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    | standalone_query_generation_llm,\n",
    "}\n",
    "# Now we retrieve the documents\n",
    "retrieved_documents = {\n",
    "    \"docs\": itemgetter(\"standalone_question\") | retriever,\n",
    "    \"question\": lambda x: x[\"standalone_question\"],\n",
    "}\n",
    "# Now we construct the inputs for the final prompt\n",
    "final_inputs = {\n",
    "    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "}\n",
    "# And finally, we do the part that returns the answers\n",
    "answer = {\n",
    "    \"answer\": final_inputs | ANSWER_PROMPT | response_generation_llm,\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "    \"context\": final_inputs[\"context\"]\n",
    "}\n",
    "# And now we put it all together!\n",
    "final_chain = loaded_memory | standalone_question | retrieved_documents | answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdec6aad-8836-4a22-a09c-62f0e283d79e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def call_conversational_rag(question, chain, memory):\n",
    "    \"\"\"\n",
    "    This function calls a conversational RAG (Retrieval-Augmented Generation) model to generate an answer to a given question.\n",
    "\n",
    "    This function sends a question to the RAG model, retrieves the answer, and stores the question-answer pair in memory \n",
    "    for context in future interactions.\n",
    "\n",
    "    Parameters:\n",
    "    question (str): The question to be answered by the RAG model.\n",
    "    chain (LangChain object): An instance of LangChain which encapsulates the RAG model and its functionality.\n",
    "    memory (Memory object): An object used for storing the context of the conversation.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the generated answer from the RAG model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare the input for the RAG model\n",
    "    inputs = {\"question\": question}\n",
    "\n",
    "    # Invoke the RAG model to get an answer\n",
    "    result = chain.invoke(inputs)\n",
    "    \n",
    "    # Save the current question and its answer to memory for future context\n",
    "    memory.save_context(inputs, {\"answer\": result[\"answer\"]})\n",
    "    \n",
    "    # Return the result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757549b7-0ba7-4577-85f7-70b90290b40e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"When does Resident Evil 4 come out on Mac and iOS?\"\n",
    "call_conversational_rag(question, final_chain, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b9e003-36d1-4355-87a6-498416cee446",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"What is the first Switch game releasing in December 2023?\"\n",
    "call_conversational_rag(question, final_chain, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3561185-0c75-437e-800c-d46665ba2c75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"What anime-influenced brawler relases on December 14?\"\n",
    "call_conversational_rag(question, final_chain, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7b807a-319c-40be-b198-cd0c3d947f02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"What game did I originally ask about?\"\n",
    "call_conversational_rag(question, final_chain, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac130f9-a8d3-443d-8d5b-0744d29a5bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-gpu.m114",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-gpu:m114"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
